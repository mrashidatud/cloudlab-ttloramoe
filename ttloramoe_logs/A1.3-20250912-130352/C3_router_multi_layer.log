=== TTLoRA A1.3 â€“ C3 (router=multi_layer) ===
git rev: b3c803f   experts: sst2 qqp mrpc cola winogrande_l rte
Fri Sep 12 13:04:02 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100S-PCIE-32GB          Off | 00000000:21:00.0 Off |                    0 |
| N/A   23C    P0              24W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100S-PCIE-32GB          Off | 00000000:E2:00.0 Off |                    0 |
| N/A   21C    P0              24W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
[RUN] python /tmp/tmp.Vbiks18kjG --batchsize 32 --epochs 100 --patience 10 --workers 8 --gpus 2 --router multi_layer
/local/miniconda/lib/python3.13/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/tmp/tmp.Vbiks18kjG", line 24, in <module>
    from moe_lightning_model import CustomLightningModule
ModuleNotFoundError: No module named 'moe_lightning_model'
