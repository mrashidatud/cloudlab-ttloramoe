=== TTLoRAMoE A1.2 single-dataset training (LoRA / TT-LoRA / Adapters) ===
Repo: /local/TTLoRAMoE-SC25
Env : ttloramoe (activation attempted; no installs)
GPUs: 2 (cap 4)  Workers: 8  Epochs: 100  Patience: 10
Batchsize: 32 (forced for all datasets)
Datasets: qqp

--- nvidia-smi ---
Thu Sep 11 23:24:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100S-PCIE-32GB          Off | 00000000:21:00.0 Off |                    0 |
| N/A   23C    P0              24W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100S-PCIE-32GB          Off | 00000000:E2:00.0 Off |                    0 |
| N/A   21C    P0              24W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+

--- git rev-parse (repo state) ---
b3c803f

[RUN] Adapter  dataset=qqp  bs=32  gpus=2  workers=8  epochs=100  patience=10
----- BEGIN Adapter (qqp) -----
/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Map:   0%|          | 0/390965 [00:00<?, ? examples/s]Map: 100%|██████████| 390965/390965 [01:12<00:00, 5426.15 examples/s]Map: 100%|██████████| 390965/390965 [01:16<00:00, 5100.28 examples/s]
There are adapters available but none are activated for the forward pass.
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Map:   0%|          | 0/390965 [00:00<?, ? examples/s]Map: 100%|██████████| 390965/390965 [01:07<00:00, 5796.18 examples/s]Map: 100%|██████████| 390965/390965 [01:12<00:00, 5414.84 examples/s]
There are adapters available but none are activated for the forward pass.
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name     | Type               | Params | Mode 
--------------------------------------------------------
0 | model    | LlamaAdapterModel  | 1.2 B  | eval 
1 | val_f1   | MulticlassF1Score  | 0      | train
2 | test_f1  | MulticlassF1Score  | 0      | train
3 | val_acc  | MulticlassAccuracy | 0      | train
4 | test_acc | MulticlassAccuracy | 0      | train
--------------------------------------------------------
12.6 M    Trainable params
1.2 B     Non-trainable params
1.2 B     Total params
4,993.769 Total estimated model params size (MB)
653       Modules in train mode
12        Modules in eval mode
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.93it/s]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
                                                                           /local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 12 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0:   0%|          | 0/5686 [00:00<?, ?it/s]Epoch 0:   0%|          | 1/5686 [00:00<1:26:45,  1.09it/s]Epoch 0:   0%|          | 1/5686 [00:00<1:26:49,  1.09it/s, v_num=36, train_loss=1.280][rank1]: Traceback (most recent call last):
[rank1]:   File "/local/TTLoRAMoE-SC25/Artifact_1.2/single_Adapter_training.py", line 168, in <module>
[rank1]:     main()
[rank1]:   File "/local/TTLoRAMoE-SC25/Artifact_1.2/single_Adapter_training.py", line 156, in main
[rank1]:     analysis =  train_moe_without_ray(config)
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/TTLoRAMoE-SC25/Artifact_1.2/single_Adapter_training.py", line 104, in train_moe_without_ray
[rank1]:     trainer.fit(model=lightning_model,
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank1]:     return function(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
[rank1]:     results = self._run_stage()
[rank1]:               ^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
[rank1]:     self.fit_loop.run()
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
[rank1]:     self.advance()
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
[rank1]:     self.epoch_loop.run(self._data_fetcher)
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
[rank1]:     self.advance(data_fetcher)
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
[rank1]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
[rank1]:     self._optimizer_step(batch_idx, closure)
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
[rank1]:     call._call_lightning_module_hook(
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
[rank1]:     optimizer.step(closure=optimizer_closure)
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
[rank1]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank1]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py", line 273, in optimizer_step
[rank1]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank1]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py", line 79, in optimizer_step
[rank1]:     closure_result = closure()
[rank1]:                      ^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
[rank1]:     self._result = self.closure(*args, **kwargs)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
[rank1]:     step_output = self._step_fn()
[rank1]:                   ^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
[rank1]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank1]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank1]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 641, in __call__
[rank1]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank1]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1632, in forward
[rank1]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank1]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1523, in _pre_forward
[rank1]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank1]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: It looks like your LightningModule has parameters that were not used in producing the loss returned by training_step. If this is intentional, you must enable the detection of unused parameters in DDP, either by setting the string value `strategy='ddp_find_unused_parameters_true'` or by setting the flag in the strategy with `strategy=DDPStrategy(find_unused_parameters=True)`.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/local/TTLoRAMoE-SC25/Artifact_1.2/single_Adapter_training.py", line 168, in <module>
[rank0]:     main()
[rank0]:   File "/local/TTLoRAMoE-SC25/Artifact_1.2/single_Adapter_training.py", line 156, in main
[rank0]:     analysis =  train_moe_without_ray(config)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/TTLoRAMoE-SC25/Artifact_1.2/single_Adapter_training.py", line 104, in train_moe_without_ray
[rank0]:     trainer.fit(model=lightning_model,
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
[rank0]:     results = self._run_stage()
[rank0]:               ^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
[rank0]:     self.advance()
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
[rank0]:     self._optimizer_step(batch_idx, closure)
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
[rank0]:     call._call_lightning_module_hook(
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
[rank0]:     optimizer.step(closure=optimizer_closure)
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/ddp.py", line 273, in optimizer_step
[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py", line 79, in optimizer_step
[rank0]:     closure_result = closure()
[rank0]:                      ^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
[rank0]:     self._result = self.closure(*args, **kwargs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
[rank0]:     step_output = self._step_fn()
[rank0]:                   ^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
[rank0]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 641, in __call__
[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1632, in forward
[rank0]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/local/miniconda/envs/ttloramoe/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1523, in _pre_forward
[rank0]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank0]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: It looks like your LightningModule has parameters that were not used in producing the loss returned by training_step. If this is intentional, you must enable the detection of unused parameters in DDP, either by setting the string value `strategy='ddp_find_unused_parameters_true'` or by setting the flag in the strategy with `strategy=DDPStrategy(find_unused_parameters=True)`.
Epoch 0:   0%|          | 1/5686 [00:06<9:41:18,  0.16it/s, v_num=36, train_loss=1.280]Command exited with non-zero status 1
WALL_SECONDS=177.15
----- END Adapter (qqp) -----

